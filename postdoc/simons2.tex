\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{pdfsync}
\usepackage[paper=a4paper,
            %marginparwidth=30.5mm,    % Length of section titles
            marginparsep=3mm,       % Space between titles and text
            margin=23mm,
            tmargin=0mm,
            ]{geometry}
\usepackage{natbib}
\setlength{\bibsep}{0.1em}

\linespread{1.25}
\usepackage[all]{nowidow}

\title{Academic Statement}
\author{Thomas Dybdahl Ahle}
\date{September 2018}
\begin{document}
\maketitle

% summarizing his or her Ph.D. thesis work
% written for a general scientific audience

Images, sound or genomic sequences are all objects that modern computers can handle due to one key idea: the idea of high dimensional geometry.
%The key idea that has let today's computers efficiently work with complicated objects like images, sound, or genomic sequences, is the idea of high dimensional geometry.
Even something as diverse as a tweet can be written as a (long) vector where the $i$th entry denotes how many copies the tweet contains of the $i$th word in the English language.
Looking at data as points in a geometry allows us to use the concept of distance to represent similarity, and of isoperimetry to understand inherent structure in data domains.
Different applications lead to different geometries, but only recently did we fully understand the most basic one -- the Euclidean geometry.

% Connects to rich mathematical areas:
% Space partitions and isoperimetry: whatâ€™s the body with least perimeter?
% Metric embeddings: can we map some geometries into others well?
% Only recently we (think we) understood the Euclidean metric
% Properties of many other geometries remain unsolved!

In my thesis, I worked on multiple problems related to search and data structures.
In~\cite{ahle2016complexity} we showed that there are certain limits to how fast you can search in any geometry.
We proved that the existence of a very fast algorithm would allow other too fast algorithms in the fundamental computational area of boolean satisfiability, breaking the so-called Strong Exponential Time Hypothesis.
The work was extended in~\cite{DBLP:journals/corr/AbboudR17} starting the field of approximate hardness for polynomial algorithms.
% (SETH) very long-standing conjectures.
%This work was later extended by the seminal work~\cite{DBLP:journals/corr/AbboudR17}, which today lays the foundation for all approximative SETH lower bounds.
%
Another key idea in high dimensional algorithms is randomization.
%It turns out that the shapes and patterns that best organize even simple data are very hard to describe and program into a computer.
It turns out that even simple high dimensional problems, like packing spheres tightly, are very hard mathematically.
Meanwhile simply placing the spheres at random locations works very well,
this has problems related to the predictability and fairness of the computation.
Today all the most efficient algorithms in the field have some chance of failing without any chance of verifying their results!
In~\cite{ahle2017optimal} I found a way to solve this problem for the two most common geometries, and in~\cite{wei2019optimal} the results were extended to cover the Euclidean case as well.
%
My most recent work includes a unified approach to LSH for distances on set/boolean data and explicit feature embeddings of polynomial kernels, giving new state of the art results in linear methods in machine learning.

%Sketching?
%Omri doesn't have anything about that, but maybe other people at Columbia do?

\paragraph{Research Plan}

At Columbia, I will tackle the most important problem in search: Edit Distance.
The edit distance between two strings like SIMON and SALOON is the minimum number of insertions, deletions or substitutions required to turn one into the other.
In this particular case, the answer is 3: delete an O and change AL into SI.
Given a database of strings and a query, we would like to quickly retrieve the most similar string in the database.
This problem is deeply linked to natural language processing
~\cite{sidorov2015computing}
% also punyakanok2004natural
and computational biology
~\cite{mcgrane2016biological},
and as of yet a very big computational mystery.

Practical solutions include transforming strings into sets (using so-called $k$-mers) in which case my recent work~\cite{ahle2019subsets} gives the state of the art algorithm.
Another approach embeds the strings into a geometry called L1
%~\cite{jowhari2012efficient, ostrovsky2005low}
which is related to the Euclidean geometry and allows fast search.
Unfortunately, both transformations distort the edit distance a great deal, which makes the approach less than ideal.
%In fact for L1~\cite{krauthgamer2009improved} showed that there is no way around this.
%\cite{mccauley2019approximate} similarly uses embedding related methods.

I believe that a more direct approach will yield much better algorithms, leading to many breakthrough from machine learning to biology.
A very recent development of papers~\cite{
   chakraborty2018approximating,
   haeupler2019near%
}
%chakraborty2016streaming,
%mccauley2019approximate
have introduced many new ideas on edit distance,
and~\cite{andoni2018data} has shown that all geometric search can be viewed through so-called non-linear spectral cuts, giving us a more principled path for research.
I will study these quantities from both an upper and lower bound perspective.
For upper bounds, new analytical methods developed in my recent papers allow studying candidate algorithms that this far has been a mystery.
For lower bounds, new hypercontractive inequalities I have discovered give a new view at spectral cuts.
Omri Weinstein is the topmost expert on lower bounds for data structures, and working with him, and other experts and Columbia University, will make this project much more likely to succeed.

\clearpage
\phantom{,}
\vspace{1cm}
\bibliographystyle{apalike}
\bibliography{simons2}

\end{document}
