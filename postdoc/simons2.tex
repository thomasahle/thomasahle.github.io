\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{pdfsync}
\usepackage[paper=a4paper,
            %marginparwidth=30.5mm,    % Length of section titles
            marginparsep=3mm,       % Space between titles and text
            margin=25mm,
            tmargin=5mm,
            ]{geometry}

\linespread{1.25}
\usepackage[all]{nowidow}

\title{Academic Statement}
\author{Thomas Dybdahl Ahle}
\date{September 2018}
\begin{document}
\maketitle

% summarizing his or her Ph.D. thesis work
% written for a general scientific audience

The key idea that has let today's computers efficiently work with complicated objects like images, sound, or genomic sequences, is the idea of high dimensional geometry.
Even something like a tweet can be written as a \emph{long} vector where the $i$th entry specifies how many copies the tweet contains of the $i$th word in the English language.
Looking at data as points in a geometry allows us to use concepts of distance to represent similarity and isoperimetry to prove optimal ways of organizing data.
Different applications lead to different geometries, and only recently did we fully understand the most basic -- the Euclidean geometry.

% Connects to rich mathematical areas:
% Space partitions and isoperimetry: what’s the body with least perimeter?
% Metric embeddings: can we map some geometries into others well?
% Only recently we (think we) understood the Euclidean metric
% Properties of many other geometries remain unsolved!

In my thesis I worked on multiple problems related to search and data structures.
In~\cite{ahle2016complexity} we showed that there are certain limits of how fast you can search in any geometry.
We did this by showing that a very fast algorithm for any of a number of problems would allow other too fast algorithms in the fundamental computational area of boolean satisfiability, breaking the so called Strong Exponential Time Hypothesis.
% (SETH) very long-standing conjectures.
%This work was later extended by the seminal work~\cite{DBLP:journals/corr/AbboudR17}, which today lays the foundation for all approximative SETH lower bounds.
%
Another key idea in high dimensional algorithms is randomization.
%It turns out that the shapes and patterns that best organize even simple data are very hard to describe and program into a computer.
It turns out that even simple high dimensional problems like packing sphere tightly is very hard mathematically.
Meanwhile simply placing the spheres at random locations gives a near optimal tightness.
This however has problems related to the predictability and fairness of the computation.
All the most efficient algorithms in the field has some chance of failing without any chance of verifying their result!
In~\cite{ahle2017optimal} I found a way to solve this problem for the two most common geometries and in~\cite{wei2018optimal} the results were extended to cover the Euclidean case.

%In my work I achieved the optimal $n^{1/c}$ and with a general approach, which also gives the optimal query time for many other distances.
%Omri was interested in this, so he must have some related work?

%Sketching?
%Omri doesn't have anything about that, but maybe other people at Columbia do?


Besides the published work, I am currently working on a unified approach to LSH for distances on set/boolean data. This involves new hyper-geometric bounds in boolean functions. I am also working on explicit feature embeddings of polynomial kernels.

\paragraph{Research Plan}

A unified theory of geometric search.
Machine learning has brought a 

From the beautiful paper~\cite{andoni2018data} we now have a general approach to data structures for so called normed spaces.
It also handles other things via those spectral cut things.
Just need to upper and lower bound it.
I really need to read it.


We now have really promising results in LSH for symmetric norms~\cite{DBLP:journals/siamcomp/AndoniKR18}.
It is however not clear if their approximation factors are optimal, and lots of work still has to be done before this important work becomes near practical - or the trade-offs are properly understood.
Going beyond normed spacers, there are a large number of metrics we don’t have any good data structures for.
Important examples are edit distance and earth mover distance, which are prevailing in both computational biology and natural language processing.
Other metrics are implicitly induced by neural networks, and similar modern machine learning architectures.

Simple things like sparse data still needs lots of work.


Better analysis of algorithms we already know. Exciting? ...
What is Rasmus vision? Making computation fair?
Mikkel? Making randomness real?
Unification
Information theory?

% research plan if the fellowship is awarded
% vison or hypothesis
The Medium dimensional regime.
Recent results~\cite{DBLP:conf/compgeom/Chan17a} have shown that classical data structures - here kd-trees - can be analysed more tightly when the dimension is close to $\log n$.
At the same time LSH algorithms have been shown in~\cite{becker2016new} to perform somewhat better in this range.
Unifying the these two approaches is a major open problem with big implications for how data is processes in practice.
From a theoretical side, proving optimality in this range requires new, sharper bounds on the noise stability of boolean functions than what is currently known.

\emph{Deterministic LSH and limited randomness.} In most of randomized algorithms, we have a good understanding on the trade-offs between randomized and deterministic variants, and the importance of high quality random bits, k-independence, tabulation hashing etc.
A natural continuation of my work in~\cite{ahle2017optimal} is to make a completely deterministic LSH data structure with little or no loss in the various performance parameters.


\emph{Nearest Neighbours beyond LSH.}
While modern LSH data structures have been improved using so called ``data dependency''~\cite{DBLP:journals/corr/AndoniR15, DBLP:conf/stoc/AndoniNNRW18}, the basic algorithm hasn’t changed since Indyk and Motwani.
Using LSH for Approximate Closest Pair yields a $n^{1-\Omega(\epsilon)}$ algorithm, but we know that algebraic algorithms allow an $n^{1-\Omega(\epsilon^{1/3})}$ algorithm~\cite{DBLP:journals/corr/AlmanCW16}.
It is a very interesting open problem whether these techniques generalizes to data structures, or conversely, if lower bounds can be shown separating Closest Pair from Nearest Neighbour.





At Columbia I plan to work with Omri Weinstein as well as a number of world class researchers in the fields of data structures and high dimensional geometry.


What do I want to do

How do I want to do it with Omri?
Omri Weinstein is an expert in proving complexity results about data structures.

We might be able to show new results using information theory.





\bibliographystyle{apalike}
\bibliography{simons2}

\end{document}
