<html>
   <head>
      <meta charset="UTF-8" />
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <link rel='stylesheet' href='style.css' />

      <script src="https://d3js.org/d3.v5.min.js"></script>
      <script src="d3-simple-slider"></script>

      <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
      <link href="https://fonts.googleapis.com/css?family=Arsenal" rel="stylesheet">
      <link href="https://fonts.googleapis.com/css2?family=Merriweather:ital@1&display=swap" rel="stylesheet">
      <link rel="shortcut icon" type="image/png" href="favicon2.png">

      <link rel="stylesheet" href="github.min.css">
      <script src="highlight.min.js"></script>
      <script> hljs.initHighlightingOnLoad(); </script>

      <script src="tree.js"></script>
      <script src="cp.js"></script>
      <script src="plot.js"></script>
      <script src="uni.js"></script>

      <title>An Evolutionary Data Structure for Sets</title>
   </head>
   <body>
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-337734-4', 'auto');
        ga('send', 'pageview');
      </script>
      <div class='time'>4 May 2020 / <a href="https://thomasahle.com">Thomas Dybdahl Ahle</a></div>
      <!--<h1>An Evoutionary Algorithm for Finding Similarities</h1>-->
      <!--<h1>An Evoutionary Algorithm better than MinHash</h1>-->
      <!--<h1>An Evolutionary Data Structure for&nbsp;Sets</h1>-->
      <h1>How Netflix Recommendations relate to the Corona Virus outbreak</h1>
      <p class="byline">
      <!--
         <i>How does Netflix recommendation systems and DNA sequencing relate to Corona outbreak&nbsp;simulations?</i>
      -->
      <i>An Evolutionary Data Structure for&nbsp;Sets</i>

      </p>
      <noscript>
         <img src='top.png' width='800' />
      </noscript>
      <svg id='picture_tree' height=400></svg>
      <script>
         const picture_tree = new RadialTree(200);
         picture_tree.mount(d3.select('#picture_tree'));
         function restartTopTree(minimum) {
            let tries = 0;
            while (true) {
               tries++;
               picture_tree.reset(6, 0.5, 1, 2);
               for (let i = 0; i < 10; i++)
                  picture_tree.step();
               picture_tree.root.count();
               if (picture_tree.root.value >= minimum)
                  break;
            }
            picture_tree.update();
         }
         window.onload = function() {
            restartTopTree(100);
         }
      </script>
      <p style='text-align:right; position:relative; top:-60px; left:60px; background: transparent; font-size: small; margin: 0; height: 0'>
      <a onclick="restartTopTree(0)" href="#" style='color:#555'>
         ↻ Refresh</a>,&nbsp;
      <a href="#tree" style='color:#555'>Interactive widget</a></p>

      <p>
      Suppose you are Netflix, and you are storing the list of movies that each user has watched.
      For a simple recommendation system, given a user, you want to know which other users film lists their watched set of movies overlap with.
      You have too many users to just compare with everyone.
      You need an index &ndash; a set database.
      </p>

      <p>
      I recently published a preprint, together with Jakob Knudsen, describing <a href="https://arxiv.org/abs/1904.04045">a new, optimal data structure for set similarity search</a>.
      An interesting way to look at this is as an <a href="https://en.wikipedia.org/wiki/Evolutionary_algorithm">Evolutionary Algorithm</a> (aka. Genetic Algorithm).
      We think it's probably the first time the <i>provably best</i> way to solve a problem has been evolutionary.
      This post won't go deep into the mathematics, but will contain <a href='#python-code'>code samples</a>
      and some fun <a href="#evolution">gadgets I built to simulate it</a>.
      </p>

      <p>You should know that <a href='#sets'>sets can represent much more than movies</a>, and that this problem has been studied in Computer Science since the 70s, but for now lets focus on the concrete.</p>

      <a name="simple"></a>
      <h3>A Simple Set Database</h3>
      <p>
      Assume Alice has watched the movies
      {<span class="set">
         Extraction,
         The Rise of Skywalker,
         Dangerous Lies,
         The Half of It,
         365 Days,
         Once Upon a Time,
         The Gentlemen,
         Parasite,
         Bad Education</span>
      }.
      <!--
      We might considering storing it in alphabetic order.
      However, then another user, Bob, who have watched the exact same movies, except for <span class="set">365 Days</span> (assuming numbers go first), would be placed in a completely different location in our index, and we would never discover the similarity.
      </p>-->

      We might make a list of users for every movie that exists,
      and write down Alice in the lists corresponding to
      <span class="set">
         Extraction</span>,
      <span class="set">
         The Rise of Skywalker</span>, ... etc.

      If Bob shares just one movie with Alice, he will discover this by looking through the sets in lists corresponding to the movies he has watched himself.
      Meanwhile, if Bob and Eve have no movies in common, Bob won't waste any time on Eve, since they won't share a list.
      </p>

      <p>
      Unfortunately everybody has seen <span class="set">Titanic</span> (at least to estimated 
       to <a href="https://fivethirtyeight.com/features/how-many-people-havent-seen-titanic/">211 million American adults</a> by FiveThirtyEighthave.)
      Hence the simplest list approach doesn't save Bob much time.

      The solution is to make lists of pairs, triples or in general n-tuples of movies, rather than single movies.

      We also pick those tuples randomly, so even if there are 5 movies everyone has seen, it's unlikely to be a problem.

<!--
      For example, Bob and Alice might both store their sets in a list indexed by {<span class="set">Parasite, The Half of It, The Gentlemen</span>}.
      If we pick the triples at random,
      it's quite unlikely that two users with vastly different sets of watched movies will appear in the same lists, and so Bob and Alice can identify each other quickly.
-->
      </p>
      <p>
      We are now pretty close to the real algorithm.
      The last change we will make is to allow people to appear in lists, even if they have only watched, say, 80% of the movies in the list.
      In fact we will require close to exactly 80%, since there is also information in what movies people <i>haven't watched</i>.
      </p>
      <p>
      An example of the data structure is shown below:
      </p>

      <pre><code class="python"><!--
-->   from random import sample
   movies = [...] # 100 movies in total
   names = [...] # 200 users in total
   # Sample a lot of keys of 6 movies each
   keys = [set(sample(movies, 6)) for _ in range(1000)]
   # Sample 30 watched movies for each user
   watched = [(name, set(sample(movies, 30))) for name in names]
   for key in keys:
      # Place a user in a list if the overlap with the key is 5/6
      print(key, '→', [name for name, mvs in watched if len(key & mvs) == 5])<!--
      --></code></pre>

      <p>Which results in the following output:</p>

      <pre>
   { <span class="set">Lawrence of Arabia, Forrest Gump, The Wizard of Oz, ...</span>} → [William]
   { <span class="set">Rocky, Saving Private Ryan, Good Will Hunting, ...</span>} → []
   { <span class="set">It&#039;s a Wonderful Life, Amadeus, Sunset Blvd., ...</span>} → [Robert, Dennis]
   { <span class="set">Casablanca, The Princess Bride, Blade Runner, ...</span>} → [Brandon, Kyle]
   { <span class="set">Singin&#039; in the Rain, The Matrix, Raging Bull, ...</span>} → [Patricia, Ruth, Sarah]
   ...</pre>

      <p>Now, if we inspect the overlap between movies watched by Patricia and Ruth, we find it is <span class="math">13</span>. Nearly 50% larger than the expected <span class="math">30/100 · 30/100 · 100 = 9</span>.
      Perfect!
      We managed to organize the sets so similar ones are close together!
      </p>

      <p>
      <i>Intuition:</i>
      Why should we expect that two sets with a shared key-set have a large overlap?
      This follows from <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes' theorem</a> in statistics:
      A given key-set is more likely to appear in two sets with a large overlap.
      Therefore two sets with a large overlap are also more likely to share a key-set.
      </p>

      <p style='text-align:center'>
      <i>"But",</i> you ask, <i>"haven't we just replaced the problem of <b>finding a movie set</b> with a large overlap, with <b>finding a key-set</b> with a large overlap?"</i>
      </p>
      <p>
      This is a <b>major problem</b>, and it's likely the reason that the simple idea above hasn't been feasible to use before now.
      It turns out that we can use key-sets that are only <i>partially</i> random.
            <!--The solution is that we can choose the key-sets in a structured way, as long as they are still sufficiently random.-->
      This is what finally brings us back to the topic of Evolution and Corona outbreaks.
      </p>




      <a name="evolution"></a>
      <h3>An Evolutionary Approach to Key Generation</h3>

      <p>
      The idea of Evolutionary Algorithms is to mimic natural selection.
      Given some problem, for which good solutions are hard to come by,
      we start with a small, random population and let it evolve.
      At each generation we select only the fittest for reproduction.

      In our case it is the key-sets we want to evolve,
      and the problem is to find key-sets that have a large overlap with our set of watched movies.
      Hence we define the fitness to be the size of this overlap.

      <p>
      The following model shows what the algorithm looks like in practice:
      </p>

      <svg id='universe_plot' height=400></svg>
      <div class='buttons' id='uni_buttons'>
         <button id='uni_step_button'><i class="material-icons">skip_next</i> Step</button>
         <button id='uni_restart_button'><i class="material-icons">stop</i> Clear</button>
      </div>
      <script>
         const uniplot = new SimulationPlot();
         uniplot.mount(d3.select('#universe_plot'), d3.select('#uni_step_button'), d3.select('#uni_restart_button'), 1);
         uniplot.update();
      </script>
      <div  style='clear:both'></div>

      <p>
      The circles on the left are the universe of possible set elements.
      (The total archive of movies.)
      The blue circles are the movies we have watched.
      On the right are the key-sets that we are evolving.
      Each individual is allowed to multiply if roughly <span class="math">80%</span> of its elements are from our set.
      </p>

      <p>
      <i>Technical point:</i> When new key-sets are evolved by sampling from the universe, the randomness is shared between all users.
      This means that the total family of key-sets that can be evolved is shared between them.
      However each user has a different fitness function (the overlap with their personal set), and they focus their effort evolving the key-sets relevant to them.
      </p>
      <!--
      That 

      we need to ensure that two sets with a given overlap <span class="math">p<sub>1</sub></span> and respective densities <span class="math">p<sub>q</sub></span> and <span class="math">p<sub>u</sub></span> both "survive".
      (In the real version you can also use different thresholds <span class="math">t<sub>q</sub></span>, <span class="math">t<sub>u</sub></span> for Alice and Bob.)
      For that we need a two dimensional version of Kullback–Leibler:
      </p>
      <div style='text-align:center; margin-top: -20px; margin-bottom: 20px;'>
      <img src='f1.png' width='593' /> 
      </div>-->

      <p>
      In the following figure we show what it looks like when <b>both Alice and Bob</b> are running the evolution at the same time, on different sets (resp. blue and orange).
      We only show the key-sets that both Alice and Bob keep, though they naturally don't know each other's sets, so on their parts they have to keep as much as in the previous plot.
      </p>
      <p>
      It is naturally harder to guarantee survival of the tribe, if we have two requirements that must hold at the same time.
      In the following figure we have thus increased the branching factor to 3,
      (and the fitness threshold to 84%.)
      </p>

      <svg id='universe_plot_2' height=400></svg>
      <div class='buttons' id='uni_buttons_2'>
         <button id='uni_step_button_2'><i class="material-icons">skip_next</i> Step</button>
         <button id='uni_restart_button_2'><i class="material-icons">stop</i> Clear</button>
      </div>
      <script>
         const uniplot2 = new SimulationPlot();
         uniplot2.mount(d3.select('#universe_plot_2'), d3.select('#uni_step_button_2'), d3.select('#uni_restart_button_2'), 2);
         uniplot2.update();
      </script>
      <div  style='clear:both'></div>
      <p>
      The figure shows clearly why pairs of sets with a large overlap are likely to result in shared key-sets:
      As you may try, it is nearly impossible to evolve large key-sets without using lots of multi colored balls.
      </p>

   <a name="life"></a>
   <h3> The Tree of Life</h3>
   <p>The <a href="https://en.wikipedia.org/wiki/Tree_of_life_(biology)">Tree of Life in biology</a> illustrates how every species of life originally evolved from one ancestor.
   Like Darwin, we too can plot the ancestry of the species we evolve.</p>

      <p>
      In the first simulation, the fit individuals of each generation would double themselves.
      If we weren't constantly abandoning a large portion of them, the exponential growth would quickly make them grow to extreme numbers.
      In terms of Coronavirus jargon, this means the reproduction number (or <a href="https://www.bbc.com/news/health-52473523">R value</a>) would be greater than 1.
      </p>
      
      <p>
      It turns out, however, that with a fitness threshold of <span class="math">80%</span>, and a density of blue balls of <span class="math">25%</span> as in the example above, a branching rate of <span class="math">2</span> is exactly the right balance to ensure long time survival of the tribe, but never have it grow exponentially.
      </p>

      <p><i>Technical details:</i>
      The required branching factor for stable growth (neither exploding nor extincting) turns out to be
      related to the so called <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback–Leibler divergence</a>
      between the density of the set in the universe <span class="math">(p)</span> and the survival threshold <span class="math">(t)</span>.
      We need the branching factor to be exactly
      </p>
      <div style='text-align:center; margin-top: -20px'>
      <img src='f.png' width='386' /> <!-- 772 -->
      </div>
      <p>
      However for the real algorithm we need to ensure that some key-set survives for both Alice <b>and</b> Bob.
      That requires a two dimensional version of Kullback–Leibler, which I won't even try to state here.
      </p>
      
      <p>
      The next gadget allows you to try different "blue ball densities" and thresholds, and see how many generations the tribes survive.
      Note that if you manually modify the branching factor, the tribe will either quickly go extinct, or explode to kill your browser tab.
      </p>

      <a name="tree"></a>
      <script>
const tree_svg = d3.select('body').append('svg').attr('id', 'tree').attr('height', 600);
const form = d3.select('body').append('div').attr('id', 'cp');
      </script>

      <p>
         This simulation doesn't show us how many blue and grey balls are in each individual.
         Instead we will plot histograms of the fitness of each generation to get an idea of how stable our growth is:
      </p>

   <script>

d3.select('body').append('h2')
         .text('Generation Fitness: Number of blue balls amoung key-sets in each generation')
         .append('i')
         .lower()
         .attr('class', 'tooltip material-icons')
         .text('info')
         .append('span')
         .attr('class', 'tooltip-text')
         .text('Joy Division plot of the fitness of the individuals alive at each generation.');


const graph = new RadialTree(300);
const cp = new ControlPanel(.25, 0.81071, graph);
const plot = new JoyPlot(graph);

const plot_svg = d3.select('body').append('svg').attr('id', 'plot');

graph.mount(tree_svg);
plot.mount(plot_svg);
// hard-coding repetitions for the first value, for faster load times
cp.mount(form, 15);

      </script>

      <a name="python-code"></a>
   <h3>A Sample Python Implementation</h3>
   <p>
   While <a href="https://arxiv.org/abs/1904.04045">the arXiv paper</a>
   proves that the above method of searching sets is optimal in terms of <a href="https://en.wikipedia.org/wiki/Big_O_notation">big-O</a> time and space,
   in practice we probably need a number of hardware and dataset related optimizations to get competitive performance.
   </p>

   <p>
   It is however simple to write a toy Python version of the data structure.
   The most important piece is the method that generates the key-sets:
   </p>

      <pre><code class="python"><!--
-->def representatives(self, point):
    # Do a small number of independent repetitions to ensure success
    for seed in self.seeds:
        # Each individual stores its value and a random identifier
        individuals = [(0, seed)]
        size = 1
        # Run through k generations/levels of the process
        for l in range(1, self.k+1):
            # The branching factor might not be an integer, so we
            # round it up, while trying to keep the geometric average
            # correct
            level_size = math.ceil(self.D**l / size)
            size *= level_size
            new_individuals = []
            for v, h in individuals:
                # Evolve the individual by generating children.
                # We seed the random generator so the children chosen
                # are consistent between inserts and queries
                random.seed(h)
                for x in random.sample(range(self.U), level_size):
                    h1 = random.randrange(self.U**3)
                    v1 = v + bool(x in point)
                    # We allow the value to be "off" by a tiny bit
                    if abs(v1 - self.t*l) <= (self.t*(1-self.t)*l)**(1/3):
                        new_individuals.append((v1, h1))
            individuals = new_individuals
        yield from (h for v,h in individuals)<!--
                 --></code></pre>

      <p>
      There are a number of parameters used:
      </p>
      <ul>
         <li> <code>self.D</code> is the branching factor <span class="math">Δ</span>.</li>
         <li> <code>self.t</code> is the threshold <span class="math">t</span> requirement for key-set fitness.</li>
         <li> <code>self.k</code> is the size of the final key-sets we are trying to generate.</li>
         <li> <code>self.U</code> is the size of the universe from witch the sets are drawn (e.g. all movies in the Netflix catalog.)</li>
      </ul>

      <p>The code keeps track of the <i>value</i> (number of sampled items that came from the given set) and a <i>hash</i> of the given key-sets.
      This hash is enough to identify the individual, so, we don't have to store the elements actually sampled.
      The hash is also used to seed the random generator, which ensures that a given individual always have the same offspring - what was earlier described as "coordinated randomness".
      </p>

      <p>
      The actual methods to insert and query the data structure are incredibly simple:
      </p>

      <pre><code class="python"><!--
-->def insert(self, point):
    for rep in self.representatives(point):
        self.lists[rep].append(point)

def query(self, point):
    for rep in self.representatives(point):
        for point2 in self.lists[rep]:
            yield point2<!--
      --></code></pre>

   <p>
   For the complete code of the Python implementation, see <a href="https://github.com/thomasahle/supermajorities">the Github repository</a>.
   The only thing it includes beyond the code above, is the following initialization:
   </p>

      <pre><code class="python"><!--
-->def __init__(self, U, t):
    self.U = U
    self.t = t
    self.k = find_height(...)
    self.D = find_branch_factor(...)
    surv = estimate_survival(self.D, self.k, ...)
    self.seeds = [random.randrange(U**3) for _ in range(int(1/surv+1))]
    self.lists = collections.defaultdict(list)<!--
      --></code></pre>
      <p>
            The <code>estimate_survival</code> function runs a simple simulation, used for estimating how many individuals need to be in the first generation, to guarantee we don't get early extinction.
      Finally, the <code>self.list</code> map, stores the actual lists illustrated in <a href="#simple">the previous section</a>.
      </p>

<!--
      <p>
      Here <code>n</code> and <code>U</code> are respectively the number of points we want to store, and the size of the universe from which our sets come.
      The values <code>wq</code> and <code>wu</code> are the density of the sets used for queries and stored in the database respectively.
      The value <code>w1</code> can be used to specify how correlated we expect the results to be with the queries.
      If this value is close to <code>wq*wu</code> (the correlation between random sets), the  time per query will be close to <span class="math">Ɵ(n)</span>, since the signal to noise ratio is very low.
      With a larger <code>w1</code>we get much better performance, but we might not find sets with only a small correlation to the query.
      </p>

      <p>
      The thresholds <code>tq</code> and <code>tu</code> are specify the fraction of set elements each key-set must contain.
      By using different values for queries and stored sets respectively, we can vary the space/time trade-off of the algorithm.
      The values <code>k</code> and <code>D</code> are the total number of generations to evolve and the branching factor used at each step.
      The <code>estimate_survival</code> function runs a simple simulation, used for estimating how many individuals need to be in the first generation, to guarantee we don't get early extinction.
      Finally, the <code>self.list</code> map, stores the actual lists illustrated in <a href="#simple">the previous section</a>.
      </p>

      <p>
      The main work is done by the <code>representatives</code> method, which computes (evolves) the key-sets relevant to a given point:
      </p>
      -->
      <p>
      Finally, let's make a small toy recommendation system:
      </p>
      <pre><code class="python"><!--
-->from supermajority import Supermajority

total_movies = 100_000
watchsets = {'Alice': {0, 4, ...}, 'Bob': {1, 3, ...}, ...]

U, n = len(all_movies), len(watchsets)
# Users may not have watched the same numnber of movies. We shoud really make
# multiple Supermajority()'s to account for this, but we'll just estimate using
# the average.
wu = wq = sum(len(watched) for name, watched in watchsets) / len(watchsets) / U
# We'll focus on overlaps of roughy half the set size.
w1 = wq / 2
# q_weight controls how to balance query time to data structure space.
sm = Supermajority(U, wq, wu, w1, n, q_weight=1)

print('Building data structure')
sm.build(watchsets.vaulues())

print('Querying data structure')
query = watchsets['Bob']
cnt = collections.Counter()
for watched in sm.query(query):
    # We could weight the counts by how good an overlap there is between
    # Bob and `watched`, but here we just add everything equally
    for movie in watched:
        cnt[movie] += 1

# Print top 10 suggestions
print('Suggestions:', cnt.most_common(10))<!--
      --></code></pre>
      <p>
      You can try it out the <a href="https://www.kaggle.com/netflix-inc/netflix-prize-data">Netflix price data</a>.
      To get competitive recommendations, you probably have to account for the skewed distributions of movies watched,
      as well as weighing for the rating and time at which it was given.
      </p>

      <a name="sets"></a>
      <h3>Perspective</h3>
      <!--
      <p>
      , originally developed for the AltaVista search engine in 1997.
Often this is measured by the tweets having a large <a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard similarity</a> and solved with the 

      A common operation is "Similarity search", where (e.g.) given a tweet, we want to find all other tweets with simiar sentiment.
      Since then it has been used for clustering documents by Google and many others.
      </p>
      -->

      <p>
      Sets can be used to represent many types of data, other than lists of movies.
      <!--Sets are just collections of distinct objects.-->
      In DNA sequencing, the DNA strings are chopped into sets of small snippets (called <a href="https://en.wikipedia.org/wiki/K-mer">k-mess</a>) such that two sets with a large overlap "contains roughly the same genes".
      When I worked in Machine Learning for customer support, we would sometimes represent the customers queries as a <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag-of-words</a>, and two sets with a large overlap would be considered to be about the same issue.

      <!--In Natural language processing as well as Computer vision, so called <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag-of-words models</a> are useful ways of generating semantic representations of text and images, in the form of sets.-->
      <!--Today an alternative is to map this data into dense vectors, using perhaps a Neural Network, but in the paper we show that this "densification" will inevitably result in worse performance or recall.-->
      </p>

      <p>
      The difficulty of set similarity is related to the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">Curse of dimensionality</a>.
      <!--Naturally, many efficient data structures for sets have been developed.-->
      <!--A common operation is "Subset search", where (e.g.) given a list of movies, we want to know which users have watched all of them.-->
      Turing Award winning Ronald Rivest wrote <a href="https://people.csail.mit.edu/rivest/pubs/Riv76b.pdf">his PhD Thesis</a> on a version of this problem all the way back in 1976.
      In the late 90s the methods of
      <a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing">Locality Sensitive Hashing</a> and the <a href="https://en.wikipedia.org/wiki/MinHash">MinHash</a> were developed, which are used today in everything from search engines to Machine Learning.
      There are also Machine Learning techniques such as <a href="https://en.wikipedia.org/wiki/Word_embedding">Word embeddings</a> that "densifies" sets to vectors.
      However it turns out that one can do better by looking at the sets directly.
      </p>

      <p>It is also worth noting, that while the present algorithm works on sets, it can easily be generalized to weighted sets or multisets.
      This is done just like <a href="https://en.wikipedia.org/wiki/MinHash#Incorporating_Weights">Incorporating Weights in MinHash</a>.
      Similar techniques alows handling Neural or <a href="https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision">"Visual" Bags of Words</a> like <a href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform">SIFT</a>.
      </p>

      <p>
      <img src="Bag_of_words.jpeg" alt="A visual bag of words representation" title="Meritxell.canela / Public domain" style='display:block'/>
      <i>A picture illustrating the visual Bag-of-words model for image search, <a href="https://commons.wikimedia.org/wiki/File:Bag_of_words.JPG">credit Wikimedia&nbsp;Commons</a>.</i>
      </p>



   <a name="conclusion"></a>
   <h3>Conclusion</h3>
   <p>
      If you take anything away from this post, I hope it is that evolution can be finicky and very sensitive to parameters such as the branching factor.
      <!--There is also a good deal of luck involved.-->
      If used responsibly, however, evolutionary algorithms can be an powerful way of solving problems.
      Even undertakings as abstract as recommendation systems and set databases.
   </p>
   <p>
   The problem of search and joins on sets has been studied for 50 years.
   Various models have been suggested, such as Subset search and Jaccard similarity search.
   The method described above is the first mathematically optimal algorithm for all of those problems,
   potentially leading to vastly more efficient systems in all areas of Computer Science.
   </p>

   </body>
</html>
